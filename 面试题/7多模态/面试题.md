由于我本人并不参与多模态的开发，以下是一些收集的面试题，仅供参考。

## 介绍一下CLIP的结构
CLIP 是由 OpenAI 在 2021 年提出的预训练模型，用于评估给定图像与给定文本描述的匹配程度。该模型使用大量（约 4 亿）从网页中爬取的图像-文本对(pair)数据进行对比学习。

搜索了 50w 个 queries（query 列表主要来自英文版维基百科中至少出现 100 次的所有单词，并做了些其他补充）

为了保证每个 query 的数据相对平衡，每个 query 保留最多 2w 个（image, text）

clip使用典型的双塔模型，有两个 encoder，一个对应图片，一个对应文本，图像和文本经过各自的 encoder 后，通过简单的点乘来代表不同模态的交互（相似性）。 训练时，假设一个 batch 有 N 对（图像，文本）对，可以有 N x N 种组合方式，对比学习把原始数据集中的 N 个组合作为正样本（下图对角线），把其他的 N x N - N 种组合作为负样本

## ALBEF的主要贡献是什么？
文章的主要贡献有两个：

ALBEF 解决了多模态领域中图像和文本对齐、交互的问题。在 ALBEF 之前，多模态方法通常使用 transformer 的多模态编码器来同时编码视觉和文本特征，由于目标检测器是提前训练好的，因此视觉和文本特征并不是对齐的。图像和文本特征可能距离很远，这使得多模态编码器难以学习到它们之间的交互。为了解决这个问题，ALBEF 通过一个对比损失（也就是 CLIP 中的 ITC 损失）在进行多模态交互之前对齐图像和文本数据。

网上爬取的大量图文对通常噪声很大（图文不匹配）。ALBEF 采用动量蒸馏（momentum distillation）的自训练方法来从网络图文对数据中学习，以缓解原始数据中的噪声问题。从理论上讲，ALBEF 通过互信息最大化的角度解释了不同的多模态任务，说明不同任务实际上为图文对提供了不同的视角，类似于数据增强，使得训练得到的多模态模型能够理解不同模态下的语义，具备语义保持的能力。

## VLMO的优势是什么？
VLMo 模型的优势之一是其灵活性。在训练阶段，根据任务的不同使用不同的结构计算损失函数，并更新对应的参数。这样的训练过程需要多次模型前向计算，但在推理阶段，灵活性的优势得到了体现。对于检索类任务，可以使用单独的文本/图像编码器提取特征，提高处理效率；而对于推理类任务，可以通过图文编码器进行充分的模态交互。这种设计巧妙地解决了传统视觉-语言模型中双编码器和融合编码器之间的冲突。

## BLIP的主要贡献是什么？
文章的研究动机：

现有的预训练模型通常在理解型任务或生成型任务中表现出色，但很少有模型能够同时在这两种任务上达到优秀的性能。

现有的性能改进主要是通过扩大数据集规模并使用从网络收集的带有噪声的图像-文本对进行训练实现的。然而，网络数据集中的噪声会对模型的性能产生负面影响。

主要的贡献：
 - 统一了图像-语言的理解与生成任务
 - Bootstrap 的方式清洗网络噪声数据

## BLIP2的主要贡献是什么？
BLIP-2 通过在冻结的预训练图像编码器和冻结的预训练大语言模型之间添加一个轻量级 查询 Transformer (Query Transformer, Q-Former) 来弥合视觉和语言模型之间的模态隔阂。在整个模型中，Q-Former 是唯一的可训练模块，而图像编码器和语言模型始终保持冻结状态。
Q-Former 由两个子模块组成，这两个子模块共享相同的自注意力层:

与冻结的图像编码器交互的图像 transformer，用于视觉特征提取
文本 transformer，用作文本编码器和解码器

## InstructBLIP的主要贡献是什么？
 - 收集了 26 数据集并转化指令微调的格式
 - 改进 BLIP2 中的 Query Transformer 为 指令感知的 Query Transformer，能够抽取和给定指令相关的信息

## LLaVA的主要贡献是什么？
LLaVA 模型的结构包括两个主要组件：视觉编码器和语言模型。

视觉编码器：LLaVA 中使用的视觉编码器基于 CLIP（ViT-L/14 变体）。在 LLaVA 中，视觉编码器接收输入图像（）并使用 ViT-L/14 模型生成视觉特征（）。然后，使用可训练的投影矩阵（），将这些视觉特征转换为语言嵌入标记（）。这个投影矩阵使得图像特征能够与语言模型的词嵌入空间对齐。

语言模型：LLaVA 中使用的语言模型为 LLaMA。语言模型接收由视觉编码器生成的语言嵌入标记（）以及文本信息（）作为输入，并根据这些输入生成文本输出。

## CogVLM的主要贡献是什么？
主要思想回归 LLM 前的多模态研究思路：更大的图像编码器可能是有效的，也就是视觉优先。


## 多模态研究了哪些东西？
 - 不同模态进行对齐；
 - 不同模态进行融合；
 - 指令微调促进人机交互，数据的质量可能比数量更重要；
 - 模型设计既要保证检索任务下的高效推理，又要能够进行多模态深度融合；
 - 进入大语言模型时代后，图文理解能力的强大可能来自于大语言模型的能力；
 - 进入大语言模型时代后，视觉优先仍然是值得探索的方向，但是训练大视觉模型向来是比较困难的；
 - 想要在多模态理解的基础上扩充多模态生成能力需要设计不同模态对应的解码器；
 - 理想的框架：多模态对齐+统一的编码器+统一的解码器，一举拿下多模态理解和生成。