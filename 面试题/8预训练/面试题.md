## 解释一下数据并行
数据并行是最常见的并行形式，因为它很简单。在数据并行训练中，数据集被分割成几个碎片，每个碎片被分配到一个设备上。这相当于**沿批次（Batch）维度对训练过程进行并行化**。每个设备将持有一个完整的模型副本，并在分配的数据集碎片上进行训练。在反向传播之后，模型的梯度将被全部减少，以便在不同设备上的模型参数能够保持同步。典型的数据并行实现：PyTorch DP。

**PyTorch DP(torch.nn.DataParallel):**

Pytorch最早提供的一种数据并行方式，它基于单进程多线程进行实现的，它使用**一个进程来计算模型权重**，在每个批处理期间将数据分发到每个GPU。

DataParallel 的计算过程如下所示：

 - 将 inputs 从主 GPU 分发到所有 GPU 上。
 - 将 model 从主 GPU 分发到所有 GPU 上。
 - 每个 GPU 分别独立进行前向传播，得到 outputs。
 - 将每个 GPU 的 outputs 发回主 GPU。
 - 在主 GPU 上，通过 loss function 计算出 loss，对 loss function 求导，求出损失梯度。
 - 计算得到的梯度分发到所有 GPU 上。
 - 反向传播计算参数梯度。
 - 将所有梯度回传到主 GPU，通过梯度更新模型权重。
 - 不断重复上面的过程。

缺点：

 - **单进程多线程带来的问题：**DataParallel使用单进程多线程进行实现的，方便了信息的交换，但受困于 GIL，会带来性能开销，速度很慢。而且，只能在单台服务器（单机多卡）上使用（不支持分布式）。同时，不能使用 Apex 进行混合精度训练。
 - **效率问题:**主卡性能和通信开销容易成为瓶颈，GPU 利用率通常很低：数据集需要先拷贝到主进程，然后再分片（split）到每个设备上；权重参数只在主卡（GPU0）上更新，需要每次迭代前向所有设备做一次同步；每次迭代的网络输出需要聚集到主卡（GPU0）上。因此，通信很快成为一个瓶颈。除此之外，这将导致主卡和其他卡之间，GPU利用率严重不均衡（比如：主卡使用了10G显存，而其他卡只使用了2G显存，batch size稍微设置大一点主卡的显存就OOM了）。
 - **不支持模型并行:**由于其本身的局限性，没办法与模型并行组合使用。

**分布式数据并行（PyTorch DDP,torch.nn.DistributedDataParallel）:**

基于多进程进行实现的，每个进程都有独立的优化器，执行自己的更新过程。每个进程都执行相同的任务，并且每个进程都与所有其他进程通信。进程（GPU）之间只传递梯度，这样网络通信就不再是瓶颈。

具体流程：

 - 首先将 rank=0 进程中的模型参数广播到进程组中的其他进程；
 - 然后，每个 DDP 进程都会创建一个 **local Reducer** 来负责梯度同步。
 - 在训练过程中，每个进程从磁盘加载 batch 数据，并将它们传递到其 GPU。每个 GPU 都有自己的前向过程，完成前向传播后，**梯度在各个 GPUs 间进行 All-Reduce**，每个 GPU 都收到其他 GPU 的梯度，从而可以独自进行反向传播和参数更新。
 - 同时，每一层的梯度不依赖于前一层，所以**梯度的 All-Reduce 和后向过程同时计算**，以进一步缓解网络瓶颈。
 - 在后向过程的最后，每个节点都得到了平均梯度，这样各个 GPU 中的模型参数保持同步。

**DP 和 DDP 的主要差异有以下几点：**

 - DP 是基于单进程多线程的实现，只用于单机情况，而 DDP 是多进程实现的，每个 GPU 对应一个进程，适用于单机和多机情况，真正实现分布式训练，并且因为每个进程都是独立的 Python 解释器，DDP 避免了 GIL 带来的性能开销。
 - 参数更新的方式不同。DDP在各进程梯度计算完成之后，各进程需要将梯度进行汇总平均，然后再由 rank=0 的进程，将其广播到所有进程后，各进程用该梯度来独立的更新参数（而 DP是梯度汇总到 GPU0，反向传播更新参数，再广播参数给其他剩余的 GPU）。由于DDP各进程中的模型，初始参数一致 (初始时刻进行一次广播)，而每次用于更新参数的梯度也一致；因此，各进程的模型参数始终保持一致（而在DP中，全程维护一个 optimizer，对各个GPU上梯度进行求平均，而在主卡进行参数更新，之后再将模型参数广播到其他GPU）。相较于DP，DDP传输的数据量更少，训练更高效，不存在 DP 中负载不均衡的问题。目前，基本上 DP 已经被弃用。
 - DDP 支持模型并行，而 DP 并不支持，这意味如果模型太大单卡显存不足时，只能使用DDP。

**完全分片数据并行 (PyTorch FSDP,torch.distributed.fsdp.FullyShardedDataParallel):**

***1. DeepSpeed ZeRO***

通常来说，在模型训练的过程中，GPU上需要进行存储的参数包括了模型本身的参数、优化器状态、激活函数的输出值、梯度以及一些零时的Buffer。针对模型状态的存储优化（去除冗余），DeepSpeed 提出了 ZeRO，ZeRO 使用的方法是分片，即每张卡只存 1/N 的模型状态量，这样系统内只维护一份模型状态参数。

ZeRO对 模型状态（Model States）参数进行不同程度的分割，主要有三个不同级别：

 - ZeRO-1 **: 对优化器状态分片（Optimizer States Sharding）
 - ZeRO-2 : 对优化器状态和梯度分片（Optimizer States & Gradients Sharding）
 - ZeRO-3 : 对优化器状态、梯度分片以及模型权重参数分片（Optimizer States & Gradients & Parameters Sharding）

**ZeRO-1**：

ZeRO-1没有将模型本身进行分片，也**没有将Gradient进行分片，而是只将优化器进行分片**。训练过程与DDP类似。

1.  forward过程由每个rank的GPU独自完整的完成，然后进行backward过程。在backward过程中，梯度通过allReduce进行同步。
2.  Optimizer state 使用贪心策略基于参数量进行分片，以此确保每个rank几乎拥有相同大小的优化器内存。
3.  每个rank只负责更新当前优化器分片的部分，由于每个rank只有分片的优化器state，所以当前rank忽略其余的state。
4.  在更新过后，通过广播或者allGather的方式确保所有的rank都收到最新更新过后的模型参数。

ZeRO-1 **非常适合使用类似Adam进行优化的模型训练**，因为Adam拥有额外的参数m（momentum）与v（variance），特别是FP16混合精度训练。ZeRO-1 不适合使用SGD类似的优化器进行模型训练，因为SGD只有较少的参数内存，并且由于需要更新模型参数，导致额外的通讯成本。ZeRO-1只是解决了Optimizer state的冗余。

**ZeRO-2**：

相比于ZeRO-1，**ZeRO-2除了对optimizer state进行切分，还对Gradient进行了切分**。

像ZeRO-1一样将optimizer的参数进行分片，并安排在不同的rank上。在backward过程中，**gradients被reduce操作到对应的rank上，取代了all-reduce**，以此减少了通讯开销。 每个rank独自更新各自负责的参数。在更新操作之后，广播或allGather保证所有的ranks接收到更新后的参数。

**ZeRO-3**：

为了进一步节省更多的内存，**ZeRO-3提出进行模型参数的分片**。类似以上两种分片方式，ranks负责模型参数的切片。可以进行参数切片的原因主要有以下两点：

1.  All-Reduce操作可以被拆分为Reduce与allgather操作的结合。
2.  模型的每一层拥有该层的完整参数，并且整个层能够直接被一个GPU装下。所以计算前向的时候，除了当前rank需要的层之外，其余的层的参数可以抛弃。从这个层面上来说，Zero相当于数据并行+模型并行。

***2. Fully Sharded Data Parallel (FSDP):***

Pytorch DDP用起来简单方便，但是要求整个模型加载到一个GPU上，这使得大模型的训练需要使用额外复杂的设置进行模型分片。因此，为了打破模型分片的障碍（**包括模型参数，梯度，优化器状态**）；同时，仍然保持了数据并行的简单性，该新特性应运而生。

FSDP 是一种新型数据并行训练方法，但与传统的数据并行不同，传统的数据并行维护模型参数、梯度和优化器状态的每个 GPU 副本，而 **FSDP 将所有这些状态跨数据并行工作线程进行分片，并且可以选择将模型参数分片卸载到 CPU**。

**DDP和FSDP的区别:**

在标准的数据并行（DistributedDataParallel）训练方法中，**每个GPU上都有一个模型副本，向前和向后传递的序列只在自己的数据分片上进行运行**。在这些局部计算之后，每个局部过程的参数和优化器与其他GPU共享，以便计算全局权重更新。

而在FullyShardedDataParallel训练方法中：

-   **Model shard**：每个GPU上仅存在**模型的分片**。
-   **All-gather**：每个GPU通过all-gather从其他GPU收集所有**权重**，以在本地计算前向传播。
-   **Forward（local）**：在本地进行前向操作。前向计算和后向计算都是利用完整模型。
-   **All-gather**：然后在后向传播之前再次执行此**权重**收集。
-   **Backward（local）**：本地进行后向操作。前向计算和后向计算都是利用完整模型，此时每个GPU上也都是**全部梯度**。
-   **Reduce-Scatter**：在向后传播之后，局部**梯度**被聚合并且通过 Reduce-Scatter 在各个GPU上分片，每个分片上的梯度是聚合之后本分片对应的那部分。
-   **Update Weight（local）**：每个GPU更新其局部**权重**分片。

同时，为了最大限度地提高内存效率，我们可以在每层前向传播后丢弃全部权重，为后续层节省内存。这可以通过将 FSDP 包装应用于网络中的每一层来实现（通过设置`reshard_after_forward=True`）。


## 解释一下模型并行
通常有两种类型的模型并行：**张量并行**和**流水线并行**。模型并行，即模型被分割并分布在一个设备阵列上。

 -  **张量并行是在一个操作中进行并行计算**，如：矩阵-矩阵乘法。
 -  **流水线并行是在各层之间进行并行计算**。

1. 张量并行：

张量并行训练是**将一个张量沿特定维度分成 N 块，每个设备只持有整个张量的 1/N，同时不影响计算图的正确性**。这需要额外的通信来确保结果的正确性。

以一般的矩阵乘法为例，假设我们有 `C = AB`。我们可以将B沿着列分割成 `[B0 B1 B2 ... Bn]`，每个设备持有一列。然后我们将 A 与每个设备上 B 中的每一列相乘，我们将得到 `[AB0 AB1 AB2 ... ABn] `。此刻，每个设备仍然持有一部分的结果，例如，设备(rank=0)持有 AB0。为了确保结果的正确性，我们需要收集全部的结果，并沿列维串联张量。通过这种方式，我们能够将张量分布在设备上，同时确保计算流程保持正确。

2. 流水线并行：

流水线并行的核心思想是，**模型按层分割成若干块，每块都交给一个设备**。
 - 在前向传播过程中，每个设备将中间的激活传递给下一个阶段。
 - 在后向传播过程中，每个设备将输入张量的梯度传回给前一个流水线阶段。

这允许设备同时进行计算，从而增加训练的吞吐量。
流水线并行训练的一个明显**缺点是训练设备容易出现空闲状态**（因为后一个阶段需要等待前一个阶段执行完毕），导致计算资源的浪费，加速效率没有数据并行高。

## 解释一下优化器并行的方法
目前随着模型越来越大，**单个GPU的显存目前通常无法装下那么大的模型了**。那么就要想办法对占显存的地方进行优化。

通常来说，模型训练的过程中，GPU上需要进行存储的参数包括了模型本身的参数、优化器状态、激活函数的输出值、梯度以及一些零时的Buffer。当进行混合精度运算时，其中模型状态参数(优化器状态 + 梯度+ 模型参数）占到了一大半以上。因此，我们**需要想办法去除模型训练过程中的冗余数据。**

而优化器相关的并行就是一种去除冗余数据的并行方案，目前这种并行最流行的方法是 **ZeRO**（即零冗余优化器）。针对模型状态的存储优化（去除冗余），**ZeRO使用的方法是分片，即每张卡只存 1/N 的模型状态量，这样系统内只维护一份模型状态**。ZeRO有三个不同级别，对模型状态进行不同程度的分片：

 - ZeRO-1 : 对优化器状态分片（Optimizer States Sharding）
 - ZeRO-2 : 对优化器状态和梯度分片（Optimizer States & Gradients Sharding）
 - ZeRO-3 : 对优化器状态、梯度分片以及模型权重参数分片（Optimizer States & Gradients & Parameters Sharding）

## moe模型的并行是什么
综合使用了数据并行和模型并行，专门为每个专家分配数据。

## 训练时如何查看你的显卡占用率，有哪些命令是你训练时会用到的？
评估你的显卡利用率：

1.  **flops比值法**：**`gpu利用率 = 实测的flops/显卡理论上的峰值flops`**。deepspeed实测flops 100t flops，而用的是A100卡理论峰值312t flops，可以得到GPU利用率只有 32.05%。
2.  **throughout估计法**：`吞吐量 = example数量/秒/GPU * max_length`；**`gpu利用率 = 实际吞吐量 / 论文中的吞吐量（假设利用率100%）`**，实测训练时处理样本速度为 3 example/s，一共有4卡，max length 2048，则吞吐量为 1536 token/s/gpu，根据llama论文可以得知，他们训练7B模型的吞吐量约为 3300 token/s/gpu，那么GPU利用率只有46.54%
3.  **torch profiler分析法**：利用torch profiler记录各个函数的时间，将结果在tensorboard上展示，在gpu kenel视图下，可以看到tensor core的利用率，比如30%。

训练时常用的命令：
 - 查看多机训练时的网速： iftop -i eth2 -n  -P
 - 查看服务器上的多卡之间的NVLINK topo：nvidia-smi topo -m
 - 查看训练时的 flops：deepspeed的配置文件中有一个参数，可以打开flops的统计，可以看到每个卡的flops，然后可以计算出整个模型的flops。
```json
{
  "flops_profiler": {
    "enabled": true,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
    }
}
```
 - 查看 deepspeed 的环境配置是否正确： ds_report

## 