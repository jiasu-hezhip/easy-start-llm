## 解释一下数据并行
数据并行是最常见的并行形式，因为它很简单。在数据并行训练中，数据集被分割成几个碎片，每个碎片被分配到一个设备上。这相当于**沿批次（Batch）维度对训练过程进行并行化**。每个设备将持有一个完整的模型副本，并在分配的数据集碎片上进行训练。在反向传播之后，模型的梯度将被全部减少，以便在不同设备上的模型参数能够保持同步。典型的数据并行实现：PyTorch DP。




## 解释一下模型并行
通常有两种类型的模型并行：**张量并行**和**流水线并行**。模型并行，即模型被分割并分布在一个设备阵列上。

 -  **张量并行是在一个操作中进行并行计算**，如：矩阵-矩阵乘法。
 -  **流水线并行是在各层之间进行并行计算**。

1. 张量并行：

张量并行训练是**将一个张量沿特定维度分成 N 块，每个设备只持有整个张量的 1/N，同时不影响计算图的正确性**。这需要额外的通信来确保结果的正确性。

以一般的矩阵乘法为例，假设我们有 `C = AB`。我们可以将B沿着列分割成 `[B0 B1 B2 ... Bn]`，每个设备持有一列。然后我们将 A 与每个设备上 B 中的每一列相乘，我们将得到 `[AB0 AB1 AB2 ... ABn] `。此刻，每个设备仍然持有一部分的结果，例如，设备(rank=0)持有 AB0。为了确保结果的正确性，我们需要收集全部的结果，并沿列维串联张量。通过这种方式，我们能够将张量分布在设备上，同时确保计算流程保持正确。

2. 流水线并行：

流水线并行的核心思想是，**模型按层分割成若干块，每块都交给一个设备**。
 - 在前向传播过程中，每个设备将中间的激活传递给下一个阶段。
 - 在后向传播过程中，每个设备将输入张量的梯度传回给前一个流水线阶段。

这允许设备同时进行计算，从而增加训练的吞吐量。
流水线并行训练的一个明显**缺点是训练设备容易出现空闲状态**（因为后一个阶段需要等待前一个阶段执行完毕），导致计算资源的浪费，加速效率没有数据并行高。

## 解释一下优化器并行的方法
目前随着模型越来越大，**单个GPU的显存目前通常无法装下那么大的模型了**。那么就要想办法对占显存的地方进行优化。

通常来说，模型训练的过程中，GPU上需要进行存储的参数包括了模型本身的参数、优化器状态、激活函数的输出值、梯度以及一些零时的Buffer。当进行混合精度运算时，其中模型状态参数(优化器状态 + 梯度+ 模型参数）占到了一大半以上。因此，我们**需要想办法去除模型训练过程中的冗余数据。**

而优化器相关的并行就是一种去除冗余数据的并行方案，目前这种并行最流行的方法是 **ZeRO**（即零冗余优化器）。针对模型状态的存储优化（去除冗余），**ZeRO使用的方法是分片，即每张卡只存 1/N 的模型状态量，这样系统内只维护一份模型状态**。ZeRO有三个不同级别，对模型状态进行不同程度的分片：

 - ZeRO-1 : 对优化器状态分片（Optimizer States Sharding）
 - ZeRO-2 : 对优化器状态和梯度分片（Optimizer States & Gradients Sharding）
 - ZeRO-3 : 对优化器状态、梯度分片以及模型权重参数分片（Optimizer States & Gradients & Parameters Sharding）

## moe模型的并行是什么
综合使用了数据并行和模型并行，专门为每个专家分配数据。

## 训练时如何查看你的显卡占用率，有哪些命令是你训练时会用到的？
评估你的显卡利用率：

1.  **flops比值法**：**`gpu利用率 = 实测的flops/显卡理论上的峰值flops`**。deepspeed实测flops 100t flops，而用的是A100卡理论峰值312t flops，可以得到GPU利用率只有 32.05%。
2.  **throughout估计法**：`吞吐量 = example数量/秒/GPU * max_length`；**`gpu利用率 = 实际吞吐量 / 论文中的吞吐量（假设利用率100%）`**，实测训练时处理样本速度为 3 example/s，一共有4卡，max length 2048，则吞吐量为 1536 token/s/gpu，根据llama论文可以得知，他们训练7B模型的吞吐量约为 3300 token/s/gpu，那么GPU利用率只有46.54%
3.  **torch profiler分析法**：利用torch profiler记录各个函数的时间，将结果在tensorboard上展示，在gpu kenel视图下，可以看到tensor core的利用率，比如30%。

训练时常用的命令：
 - 查看多机训练时的网速： iftop -i eth2 -n  -P
 - 查看服务器上的多卡之间的NVLINK topo：nvidia-smi topo -m
 - 查看训练时的 flops：deepspeed的配置文件中有一个参数，可以打开flops的统计，可以看到每个卡的flops，然后可以计算出整个模型的flops。
```json
{
  "flops_profiler": {
    "enabled": true,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
    }
}
```
 - 查看 deepspeed 的环境配置是否正确： ds_report

## 