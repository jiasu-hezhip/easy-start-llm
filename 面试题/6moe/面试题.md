note：现在一般不会要你自己训练moe，主要是因为moe模型对显存的要求确实非常高，即便是2bit的量化，一个8*7B的模型想要运行也需要16G左右的显存，正常进行4bit量化需要27G左右显存，已经不是单24G显卡可以玩的了。不过预训练速度更快，推理速度更快。

## 什么是moe模型？讲讲它的原理
MOE（Mixture of Experts）通过组合多个专家模型来提高模型效率和性能。在大语言模型中，MOE被用于实现更高效的并行计算和模型扩展，尤其是在处理非常大的数据集和参数规模时。

MOE的基本原理:

MOE模型的核心思想是将复杂的任务分解给不同的“专家”模型来处理，每个专家模型专注于处理特定类型的输入或任务。这样做的好处是可以利用不同专家模型的优势，同时减少不必要的计算开销。

MOE模型的关键组件
 - Gate（门控机制）：
门控机制决定了哪个专家模型（或者哪些专家模型）应该处理给定的输入。
门控机制通常是一个简单的神经网络，它基于输入特征预测出每个专家模型的权重，这些权重反映了各个专家模型对于该输入的重要程度。
 - Experts（专家模型）：
每个专家模型都是一个独立的子模型，用于处理特定类型的任务或数据。
在大规模语言模型中，专家模型可能是具有特定架构的小型神经网络，如Transformer层。
 - Combination（组合机制）：
根据门控机制提供的权重，将各个专家模型的输出加权合并成最终的输出。
这种加权组合确保了每个专家模型的贡献与其相关性相匹配。

推理流程:
 - 输入处理：
输入首先传递给门控机制。
 - 专家选择：
门控机制根据输入特性为每个专家模型分配一个权重。
 - 专家计算：
输入被送入被选中的专家模型进行处理。
 - 结果合并：
各个专家模型的输出按照门控机制给出的权重进行加权平均，生成最终输出。

优点:
 - 高效性：由于每个输入只由部分专家模型处理，因此减少了计算成本。
 - 可扩展性：MOE架构允许轻松添加更多专家模型，以适应更大的模型规模和更复杂的数据集。
 - 灵活性：通过调整门控机制和专家模型，可以优化模型性能以满足特定任务的需求。

**专家模型并不是预先单独训练好的**，而是在整个MOE系统的训练过程中一起学习和优化的。在原本的transformer块中，使用MoE-FFN替代了FFN。下面是一些具体的细节：

 - 初始化：
专家模型和门控机制都从随机初始化开始。
在训练开始之前，所有模型参数都是随机设置的。
 - 联合训练：
专家模型和门控机制是作为一个整体来训练的。
在训练过程中，每个专家模型以及门控机制都会根据模型的整体损失函数进行更新。
 - 动态调整：
随着训练的进行，门控机制会逐渐学会如何根据输入的特点选择合适的专家模型。
专家模型也会逐步优化自己的参数，以便更好地处理特定类型的输入。
 - 优化目标：
整个MOE模型的目标是最大化整体性能，而不是单个专家模型的表现。
训练过程中，系统试图找到一组参数，使得门控机制能够有效地选择最适合处理当前输入的专家模型，同时专家模型能够针对它们所负责的输入类型产生高质量的输出。
 - 参数共享与专用：
有些情况下，专家模型之间可能共享某些参数，但通常每个专家模型都有自己的专用参数，以便它们能够专门针对某些输入或任务进行优化。
门控机制也可能有自己的参数，用于决定如何分配输入到不同的专家模型。
 - 稀疏激活：
在训练过程中，MOE通常使用稀疏激活策略，这意味着不是所有的专家模型都会参与到每一个输入的处理中。这有助于减少计算负担，并且让每个专家模型更加专注于特定的任务或输入类型。

## moe模型根本上存在哪些问题？
1. 负载不均衡

对于不同的输入Token，专家路由（Gate函数）总是倾向于路由到少数专家。如果不采取措施，这种现象很容易发生，而且会恶性循环。

缓解负载不均衡的手段包括：
 - 在专家路由中引入随机性；
 - 在损失函数中增加负载不均衡的惩罚项，即辅助损失；
 - 在一个batch中设定每个专家处理Token的上限，即专家容量。
2. 训练不稳定

训练不稳定的原因归根到底是有限计算精度造成的数值计算问题。MoE结构的引入，增加了网络中基于指数运算的softmax计算，指数运算（通常结果数值较大）对于精度误差比较敏感，这就使得混合专家模型更容易发生训练不稳定。

缓解训练不稳定的方法包括：
 - 在专家路由的计算过程中使用全精度；
 - 用更小的方差初始化模型参数；
 - 增加针对性设计的损失函数，router z-loss。
3. 微调过拟合

因为引入MoE的结构大幅增加了模型参数，所以在有限样本上微调时混合专家模型更容易过拟合，导致在下游任务中的效果反而不如dense模型。

缓解微调过拟合的方式主要是：提高正则的强度，一种选择是在微调阶段单独为MoE层设置更大的Dropout概率。
4. 所谓的专家并不专业

有工作发现：在encoder结构中，专家显现出了一定的专业性；然而，在decoder结构中，专家并没有显现出专业性，大模型又大都是纯decoder结构。

提高专家专业性的方法包括：
 - 适当提高每个Token被分配路由专家的个数，促进专家互相解耦；
 - 在MoE结构中设置共享专家，专门处理不同Token的共性。
5. 计算效率低

在只使用数据并行时，MoE的稀疏性会降低每个Expert实际的batch size，导致计算效率降低；为此，引入了专家并行（特殊的模型并行）。

引入专家并行后，通信带宽又成为了计算效率的限制因素；为此，更要强化专家的负载均衡控制，同时限制专家容量。

6. 显存占用高，这是真无解，我理解这是一种空间换时间的做法。

## 以下是一些论文解读

[知乎论文解读1,群魔乱舞：MoE大模型详解](https://www.zhihu.com/tardis/bd/art/677638939?source_id=1001)

[知乎论文解读2，MOE 系列模型小记](https://zhuanlan.zhihu.com/p/712676995)